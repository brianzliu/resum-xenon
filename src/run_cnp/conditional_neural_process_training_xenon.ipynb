{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Neural Processes (CNP) for XENON.\n",
    "[Conditional Neural Processes](https://arxiv.org/pdf/1807.01613.pdf) (CNPs) were\n",
    "introduced as a continuation of\n",
    "[Generative Query Networks](https://deepmind.com/blog/neural-scene-representation-and-rendering/)\n",
    "(GQN) to extend its training regime to tasks beyond scene rendering, e.g. to\n",
    "regression and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: cannot import name '_print_elapsed_time' from 'sklearn.utils' (/home/tidmad/miniconda3/envs/coherent/lib/python3.12/site-packages/sklearn/utils/__init__.py). Retrying import...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import shap\n",
    "import yaml\n",
    "from resum.utilities import plotting_utils_cnp as plotting\n",
    "from resum.utilities import utilities as utils\n",
    "try:\n",
    "    from resum.conditional_neural_process import DeterministicModel\n",
    "    from resum.conditional_neural_process import DataGeneration\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}. Retrying import...\")\n",
    "    from resum.conditional_neural_process import DeterministicModel\n",
    "    from resum.conditional_neural_process import DataGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../xenon/settings.yaml\", \"r\") as f:\n",
    "    config_file = yaml.safe_load(f)\n",
    "\n",
    "TRAINING_EPOCHS = int(config_file[\"cnp_settings\"][\"training_epochs\"]) # Total number of training points: training_iterations * batch_size * max_content_points\n",
    "PLOT_AFTER = int(config_file[\"cnp_settings\"][\"plot_after\"])\n",
    "torch.manual_seed(0)\n",
    "BATCH_SIZE = config_file[\"cnp_settings\"][\"batch_size_train\"]\n",
    "FILES_PER_BATCH = config_file[\"cnp_settings\"][\"files_per_batch_train\"]\n",
    "target_range = config_file[\"simulation_settings\"][\"target_range\"]\n",
    "is_binary = target_range[0] >= 0 and target_range[1] <= 1\n",
    "\n",
    "path_out = config_file[\"path_settings\"][\"path_out_cnp\"]\n",
    "version = config_file[\"path_settings\"][\"version\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_size, y_size = utils.get_feature_and_label_size(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Augmentation in Progress: Applying transformations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 33/303 [00:05<00:42,  6.31it/s]\n",
      "/home/tidmad/bliu/resum-coherent/resum/conditional_neural_process/data_generator.py:245: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  '''\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m USE_DATA_AUGMENTATION = config_file[\u001b[33m\"\u001b[39m\u001b[33mcnp_settings\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33muse_data_augmentation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# load data:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m dataset_train = \u001b[43mDataGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mpath_to_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath_to_files_train\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m                               \u001b[49m\u001b[43muse_data_augmentation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSE_DATA_AUGMENTATION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mfiles_per_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFILES_PER_BATCH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m dataset_train.set_loader()\n\u001b[32m     29\u001b[39m dataloader_train = dataset_train.dataloader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bliu/resum-coherent/resum/conditional_neural_process/data_generator.py:197\u001b[39m, in \u001b[36mDataGeneration.__init__\u001b[39m\u001b[34m(self, mode, config_file, path_to_files, batch_size, files_per_batch, use_data_augmentation)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData Augmentation in Progress: Applying transformations...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tqdm(files):\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmixup_augment_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcnp_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muse_beta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43msignal_condition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     _phi_key=\u001b[33m\"\u001b[39m\u001b[33mphi_mixedup\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m     _target_key=\u001b[33m\"\u001b[39m\u001b[33mtarget_mixedup\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bliu/resum-coherent/resum/conditional_neural_process/data_generator.py:343\u001b[39m, in \u001b[36mDataGeneration.mixup_augment_data\u001b[39m\u001b[34m(self, filename, use_beta, condition_strings, seed)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mphi_mixedup\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m f[\u001b[33m\"\u001b[39m\u001b[33mphi_mixedup\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mphi_mixedup\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mphi_mixedup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgzip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtarget_mixedup\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m    346\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m f[\u001b[33m\"\u001b[39m\u001b[33mtarget_mixedup\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/coherent/lib/python3.12/site-packages/h5py/_hl/group.py:186\u001b[39m, in \u001b[36mGroup.create_dataset\u001b[39m\u001b[34m(self, name, shape, dtype, data, **kwds)\u001b[39m\n\u001b[32m    183\u001b[39m         parent_path, name = name.rsplit(\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    184\u001b[39m         group = \u001b[38;5;28mself\u001b[39m.require_group(parent_path)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m dsid = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m dset = dataset.Dataset(dsid)\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/coherent/lib/python3.12/site-packages/h5py/_hl/dataset.py:178\u001b[39m, in \u001b[36mmake_new_dset\u001b[39m\u001b[34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0, fill_time)\u001b[39m\n\u001b[32m    175\u001b[39m dset_id = h5d.create(parent.id, name, tid, sid, dcpl=dcpl, dapl=dapl)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[43mdset_id\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5s\u001b[49m\u001b[43m.\u001b[49m\u001b[43mALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5s\u001b[49m\u001b[43m.\u001b[49m\u001b[43mALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dset_id\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "d_x, d_in, representation_size, d_out = x_size , x_size+y_size, 32, y_size*2\n",
    "encoder_sizes = [d_in, 32, 64, 128, 128, 128, 64, 48, representation_size]\n",
    "decoder_sizes = [representation_size + d_x, 32, 64, 128, 128, 128, 64, 48, d_out]\n",
    "\n",
    "model = DeterministicModel(encoder_sizes, decoder_sizes)\n",
    "writer = SummaryWriter(log_dir=f'{path_out}/cnp_{version}_tensorboard_logs')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "# \n",
    "\n",
    "bce = nn.BCELoss()\n",
    "iter_testing = 0\n",
    "\n",
    "# create a PdfPages object\n",
    "test_idx=0\n",
    "it_batch = 0\n",
    "for it_epoch in range(TRAINING_EPOCHS):\n",
    "    \n",
    "    USE_DATA_AUGMENTATION = config_file[\"cnp_settings\"][\"use_data_augmentation\"]\n",
    "    # load data:\n",
    "    dataset_train = DataGeneration(mode = \"training\", \n",
    "                                   config_file=config_file, \n",
    "                                   path_to_files=config_file[\"path_settings\"][\"path_to_files_train\"], \n",
    "                                   use_data_augmentation=USE_DATA_AUGMENTATION, \n",
    "                                   batch_size=BATCH_SIZE,\n",
    "                                   files_per_batch=FILES_PER_BATCH)\n",
    "    dataset_train.set_loader()\n",
    "    dataloader_train = dataset_train.dataloader\n",
    "\n",
    "    dataset_test = DataGeneration(mode = \"training\", \n",
    "                                  config_file=config_file, \n",
    "                                  path_to_files=config_file[\"path_settings\"][\"path_to_files_train\"], \n",
    "                                  use_data_augmentation=False, \n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  files_per_batch=FILES_PER_BATCH)\n",
    "    dataset_test.set_loader()\n",
    "    dataloader_test = dataset_test.dataloader\n",
    "    data_iter = iter(dataloader_test)\n",
    "\n",
    "    #it_batch = 0\n",
    "    \n",
    "    for b, batch in enumerate(dataloader_train):\n",
    "        batch_formated=dataset_train.format_batch_for_cnp(batch,config_file[\"cnp_settings\"][\"context_is_subset\"] )\n",
    "        # Get the predicted mean and variance at the target points for the testing set\n",
    "        log_prob, mu, _ = model(batch_formated.query, batch_formated.target_y, is_binary)\n",
    "        \n",
    "        # Define the loss\n",
    "        loss = -log_prob.mean()\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform gradient descent to update parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "        # reset gradient to 0 on all parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if is_binary:\n",
    "            loss_bce = bce(mu, batch_formated.target_y)\n",
    "        else:\n",
    "            loss_bce=-1\n",
    "        \n",
    "        # Inside your batch loop, right after computing losses:\n",
    "        writer.add_scalar('Loss/train_logprob', loss.item(), it_batch)\n",
    "        if is_binary:\n",
    "            writer.add_scalar('Loss/train_BCE', loss_bce.item(), it_batch)\n",
    "\n",
    "        \n",
    "        mu=mu[0].detach().numpy()\n",
    "        \n",
    "        if it_batch % PLOT_AFTER == 0:\n",
    "            batch_testing = next(data_iter)\n",
    "            batch_formated_test=dataset_test.format_batch_for_cnp(batch_testing,config_file[\"cnp_settings\"][\"context_is_subset\"] )\n",
    "          \n",
    "            print('{} Iteration: {}/{}, train loss: {:.4f} (vs BCE {:.4f})'.format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),it_epoch, it_batch,loss, loss_bce))\n",
    "            \n",
    "            writer.add_scalar('Loss/train_logprob', loss.item(), iter_testing)\n",
    "            if is_binary:\n",
    "                writer.add_scalar('Loss/train_BCE', loss_bce.item(), iter_testing)\n",
    "\n",
    "            log_prob_testing, mu_testing, _ = model(batch_formated_test.query, batch_formated_test.target_y, is_binary)\n",
    "            loss_testing = -log_prob_testing.mean()\n",
    "            \n",
    "\n",
    "            if is_binary:\n",
    "                loss_bce_testing = bce(mu_testing,  batch_formated_test.target_y)\n",
    "            else:\n",
    "                loss_bce_testing = -1.\n",
    "\n",
    "            writer.add_scalar('Loss/test_logprob', loss_testing.item(), test_idx)\n",
    "            if is_binary:\n",
    "                writer.add_scalar('Loss/test_BCE', loss_bce_testing.item(), test_idx)\n",
    "\n",
    "            mu_testing = mu_testing[0].detach().numpy()\n",
    "            print(\"{}, Iteration: {}, test loss: {:.4f} (vs BCE {:.4f})\".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), it_batch, loss_testing, loss_bce_testing))\n",
    "            if y_size ==1:\n",
    "                fig = plotting.plot(mu, batch_formated.target_y[0].detach().numpy(), f'{loss:.2f}', mu_testing, batch_formated_test.target_y[0].detach().numpy(), f'{loss_testing:.2f}', target_range, it_batch)\n",
    "                writer.add_figure('Prediction/train_vs_test', fig, global_step=test_idx)\n",
    "            else:\n",
    "                for k in range(y_size):\n",
    "                    fig = plotting.plot(mu[:,k], batch_formated.target_y[0].detach().numpy()[:,k], f'{loss:.2f}', mu_testing[:,k], batch_formated_test.target_y[0].detach().numpy()[:,k], f'{loss_testing:.2f}', target_range, it_batch)\n",
    "                    writer.add_figure(f'Prediction/train_vs_test_k{k}', fig, global_step=test_idx)\n",
    "            test_idx+=1\n",
    "    \n",
    "        it_batch+=1\n",
    "\n",
    "writer.close()\n",
    "torch.save(model.state_dict(), f'{path_out}/cnp_{version}_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coherent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
